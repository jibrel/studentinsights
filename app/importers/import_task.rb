class ImportTask
  def initialize(options:)
    @options = options

    # options["school"] is an optional filter; imports all schools if nil
    @school = @options.fetch("school", nil)

    # options["source"] describes the list of importers to run
    @importer_keys = @options.fetch("source", ["x2", "star"])

    # This option controls whether older data is ignored.  Different importers
    # respond differently to these options (some do not respect them).
    @skip_old_records = @options.fetch('skip_old_records', false)

    # to skip updating any indexes after (eg, when tuning a particular job)
    @skip_index_updates = @options.fetch('skip_index_updates', false)

    @log = Rails.env.test? ? LogHelper::FakeLog.new : STDOUT
  end

  def connect_transform_import
    begin
      @record = create_import_record!
      @report = create_report
      log("Initialized from options = #{@options.to_json}...")

      log('Starting validation...')
      validate_school_options
      verify_school_definitions!
      log('Done validation.')

      log('Starting importing work...')
      @report.print_initial_counts_report
      import_all_the_data
      log('Done importing work.')

      log('Starting update tasks and final report...')
      run_update_tasks
      @report.print_final_counts_report
      log('Done.')
    rescue => err
      # Note that there is also separate error handling for each importer class independently.
      log("ImportTask aborted because of an error: #{err}")
      log("Re-raising exception...")
      Rollbar.error('ImportTask aborted because of an error', err)
      raise err
    end
  end

  private

  ## VALIDATE DEVELOPER INPUT ##
  def validate_school_options
    # Verify that the list of school local_ids that were passed (or are
    # default) exist before starting the import.
    school_ids.each { |id| School.find_by!(local_id: id) }
  end

  # Verify that the schools in the school definition files are present
  # in the database, otherwise raise.
  def verify_school_definitions!
    school_definitions = PerDistrict.new.school_definitions_for_import
    school_definitions.each do |school_definition|
      school = School.find_by!(local_id: school_definition['local_id'])
      school.assign_attributes(school_definition)
      if school.changed?
        raise "School id:#{school.id}, local_id:#{school.local_id} does not match definition: #{school_definition.as_json}"
      end
    end
  end

  def school_ids
    return @school if @school.present?

    PerDistrict.new.school_definitions_for_import.map { |school| school["local_id"] }
  end

  ## SET UP COMMAND LINE REPORT AND DATABASE RECORD ##

  def create_import_record!
    ImportRecord.create!(
      task_options_json: @options.to_json,
      time_started: DateTime.current,
    )
  end

  def create_report
    models = [
      Student,
      StudentAssessment,
      DisciplineIncident,
      Absence,
      Tardy,
      Educator,
      School,
      Course,
      Section,
      StudentSectionAssignment,
      EducatorSectionAssignment
    ]
    ImportTaskReport.new(
      models_for_report: models,
      record: @record,
      log: @log,
    )
  end

  ## IMPORT ALL THE DATA ##

  def options_for_file_importers
    {
      school_scope: school_ids,
      log: @log,
      skip_old_records: @skip_old_records
    }
  end

  # This is the main method doing all the work
  def import_all_the_data
    log('Starting #import_all_the_data...')
    file_import_classes = FileImporterOptions.new.prioritized_file_importer_classes(@importer_keys)
    log("file_import_classes = [\n  #{file_import_classes.join("\n  ")}\n]")
    timing_log = []

    file_import_classes.each do |file_import_class|
      file_importer = file_import_class.new(options: options_for_file_importers)

      timing_data = { importer: file_importer.class.name, start_time: Time.current }

      begin
        log("Starting file_importer#import for #{file_importer.class}...")
        file_importer.import
        log("Done file_importer#import for #{file_importer.class}.")
      rescue => error
        log("Error! #{error}")
        log(error.backtrace)

        message = "ImportTask#import_all_the_data caught an error from #{file_import_class.name} and aborted that import task, but is continuing the job..."
        log(message)
        Rollbar.error(message, error)
      end

      timing_data[:end_time] = Time.current
      timing_log << timing_data

      @record.update(
        importer_timing_json: timing_log.to_json,
        time_ended: DateTime.current,
      )

      @record.save!
    end
    log('Done #import_all_the_data...')
  end

  ## RUN TASKS THAT CACHE DATA ##

  def run_update_tasks
    if @skip_index_updates
      log('Skipping index updates...')
      return
    end

    begin
      log('Calling SchoolScopeMigrator#migrate_all!')
      SchoolScopeMigrator.new(log: @log).migrate_all!

      log('Calling Student.update_recent_student_assessments!...')
      Student.update_recent_student_assessments!

      log('Calling PrecomputeStudentHashesJob.precompute_all!...')
      PrecomputeStudentHashesJob.new(@log).precompute_all!

      log('Done index updates.')
    rescue => error
      Rollbar.error('ImportTask#run_update_tasks', error)
      raise error
    end
  end

  ## LOGGING STUFF ##

  def log(msg)
    full_msg = "\n\nðŸ’¾  ImportTask: #{msg}"
    @log.puts full_msg
    @log.flush # prevent buffering, this seems to be a problem in production jobs
    @record.log += full_msg
    @record.save!
  end
end
